{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as python_random\n",
    "import tensorflow as tf\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(123)\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "python_random.seed(123)\n",
    "# The below set_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n",
    "tf.random.set_seed(1234)\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, mean_squared_error\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "例1： model: call或者layer:call里面对一个batchadd_loss, add_metric相当于metric.update_state(y_batch, y_pred_batch)。\n",
    "     但是add_loss会被自动重置，也就是每个batch结束后self.loss得到的结果就是这个batch的loss\n",
    "        add_metric是累计的结果，也就是每个batch结束后self.metrics里面得到结果是累积的，需要手动重置（一般是一个epoch结束后）\n",
    "'''\n",
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "  def __init__(self, name=None):\n",
    "    super(LogisticEndpoint, self).__init__(name=name)\n",
    "    self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "  def call(self, targets, logits, sample_weights=None):\n",
    "    # Compute the training-time loss value and add it\n",
    "    # to the layer using `self.add_loss()`.\n",
    "    loss = self.loss_fn(targets, logits, sample_weights)\n",
    "    self.add_loss(loss)\n",
    "\n",
    "    # Log accuracy as a metric and add it\n",
    "    # to the layer using `self.add_metric()`.\n",
    "    acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "    self.add_metric(acc, name=\"accuracy\")\n",
    "\n",
    "    # Return the inference-time prediction tensor (for `.predict()`).\n",
    "    return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current accuracy value: 0.0\n",
      "current loss: [<tf.Tensor: shape=(), dtype=float32, numpy=0.6931472>]\n"
     ]
    }
   ],
   "source": [
    "targets = tf.ones((2, 1))\n",
    "logits = tf.zeros((2, 1))\n",
    "layer = LogisticEndpoint()\n",
    "y = layer(targets, logits)\n",
    "print(\"current accuracy value:\", float(layer.metrics[0].result()))\n",
    "print(\"current loss:\", layer.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current accuracy value: 0.5\n",
      "current loss: [<tf.Tensor: shape=(), dtype=float32, numpy=0.31326172>]\n"
     ]
    }
   ],
   "source": [
    "targets = tf.ones((2, 1))\n",
    "logits = tf.ones((2, 1))\n",
    "y = layer(targets, logits)\n",
    "print(\"current accuracy value:\", float(layer.metrics[0].result()))\n",
    "print(\"current loss:\", layer.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current accuracy value: 0.6666666865348816\n",
      "current loss: [<tf.Tensor: shape=(), dtype=float32, numpy=0.31326172>]\n"
     ]
    }
   ],
   "source": [
    "targets = tf.ones((2, 1))\n",
    "logits = tf.ones((2, 1))\n",
    "y = layer(targets, logits)\n",
    "print(\"current accuracy value:\", float(layer.metrics[0].result()))\n",
    "print(\"current loss:\", layer.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loss：无论是计算每个样本loss的底层函数,e.g.,keras.losses.binary_crossentropy\n",
    "# 还是计算整个batch的高层函数，e.g.,tf.keras.losses.BinaryCrossentropy，都是每次调用后自动重置的\n",
    "y_true = [[0], [0], [1], [0]]\n",
    "y_pred = [[0.2], [0.5], [0.3], [0.3]]\n",
    "bce = keras.losses.BinaryCrossentropy()\n",
    "mse = keras.losses.MeanSquaredError()\n",
    "print('bce_instance', bce(y_true, y_pred).numpy())\n",
    "print('bce_instance', bce(y_true, y_pred).numpy())\n",
    "print('bce_fn_reduce_mean', tf.reduce_mean(keras.losses.binary_crossentropy(y_true, y_pred)))\n",
    "print('bce_fn_reduce_mean', tf.reduce_mean(keras.losses.binary_crossentropy(y_true, y_pred)))\n",
    "print('mse_instance', mse(y_true, y_pred).numpy())\n",
    "print('mse_instance', mse(y_true, y_pred).numpy())\n",
    "print('mse_fn_reduce_mean', tf.reduce_mean(keras.losses.mean_squared_error(y_true, y_pred)))\n",
    "print('mse_fn_reduce_mean', tf.reduce_mean(keras.losses.mean_squared_error(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bce_fn_mask tf.Tensor([0.        0.        1.2039725 0.3566748], shape=(4,), dtype=float32)\n",
      "bce_instance_mask 0.7803236\n",
      "bce_instance_weight 0.3901618\n",
      "bce_fn_mask_reduce_mean tf.Tensor(0.3901618, shape=(), dtype=float32)\n",
      "mse_fn_mask tf.Tensor([0.         0.         0.48999998 0.09      ], shape=(4,), dtype=float32)\n",
      "mse_instance_mask 0.29\n",
      "mse_instance_weight 0.145\n",
      "mse_fn_mask_reduce_mean tf.Tensor(0.145, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# mask住一部分样本，计算损失函数\n",
    "print('bce_fn_mask', keras.losses.binary_crossentropy(y_true, y_pred) * np.array([0, 0, 1, 1]))\n",
    "# loss instance是对所有样本求平均\n",
    "print('bce_instance_mask', bce(y_true[2:], y_pred[2:]).numpy())\n",
    "# 如果利用sample_weight来mask的话，相当于只计算没被mask样本的loss，但是平均的时候还是对全部样本做平均\n",
    "print('bce_instance_weight', bce(y_true, y_pred, sample_weight=[0, 0, 1, 1]).numpy())\n",
    "print('bce_fn_mask_reduce_mean', tf.reduce_mean(keras.losses.binary_crossentropy(y_true, y_pred)* np.array([0, 0, 1, 1])))\n",
    "\n",
    "# mask住一部分样本，计算损失函数\n",
    "print('mse_fn_mask', keras.losses.mean_squared_error(y_true, y_pred) * np.array([0, 0, 1, 1]))\n",
    "# loss instance是对所有样本求平均\n",
    "print('mse_instance_mask', mse(y_true[2:], y_pred[2:]).numpy())\n",
    "# 如果利用sample_weight来mask的话，相当于只计算没被mask样本的loss，但是平均的时候还是对全部样本做平均\n",
    "print('mse_instance_weight', mse(y_true, y_pred, sample_weight=[0, 0, 1, 1]).numpy())\n",
    "print('mse_fn_mask_reduce_mean', tf.reduce_mean(keras.losses.mean_squared_error(y_true, y_pred)* np.array([0, 0, 1, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_instance 0.5\n",
      "mse_instance 0.2175\n",
      "bce_instance 0.61923444\n",
      "0.8333333\n",
      "0.1575\n",
      "0.4819079\n",
      "auc_instance 0.5\n",
      "mse_instance 0.2175\n",
      "bce_instance 0.61923444\n"
     ]
    }
   ],
   "source": [
    "# metric: update_state是累计的效应，需要手工reset_state\n",
    "y_true = [[0], [0], [1], [0]]\n",
    "y_pred = [[0.2], [0.5], [0.3], [0.3]]\n",
    "auc = keras.metrics.AUC()\n",
    "mse = keras.metrics.MeanSquaredError()\n",
    "bce = keras.metrics.BinaryCrossentropy()\n",
    "auc.update_state(y_true, y_pred, sample_weight=)\n",
    "mse.update_state(y_true, y_pred)\n",
    "bce.update_state(y_true, y_pred)\n",
    "print('auc_instance', auc.result().numpy())\n",
    "print('mse_instance', mse.result().numpy())\n",
    "print('bce_instance', bce.result().numpy())\n",
    "y_true_2 = [[1], [1], [0], [0]]\n",
    "y_pred_2 = [[0.8], [0.5], [0.3], [0.1]]\n",
    "auc.update_state(y_true_2, y_pred_2)\n",
    "mse.update_state(y_true_2, y_pred_2)\n",
    "bce.update_state(y_true_2, y_pred_2)\n",
    "print(auc.result().numpy())\n",
    "print(mse.result().numpy())\n",
    "print(bce.result().numpy())\n",
    "auc.reset_states()\n",
    "mse.reset_states()\n",
    "bce.reset_states()\n",
    "auc.update_state(y_true, y_pred)\n",
    "mse.update_state(y_true, y_pred)\n",
    "bce.update_state(y_true, y_pred)\n",
    "print('auc_instance', auc.result().numpy())\n",
    "print('mse_instance', mse.result().numpy())\n",
    "print('bce_instance', bce.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_instance_mask 0.5\n",
      "mse_instance_mask 0.29\n",
      "bce_instance_mask 0.7803236\n",
      "auc_instance_weight 0.5\n",
      "auc_instance_weight 0.29\n",
      "auc_instance_weight 0.7803236\n"
     ]
    }
   ],
   "source": [
    "# mask情况下的metric\n",
    "auc.reset_states()\n",
    "mse.reset_states()\n",
    "bce.reset_states()\n",
    "auc.update_state(y_true[2:], y_pred[2:])\n",
    "mse.update_state(y_true[2:], y_pred[2:])\n",
    "bce.update_state(y_true[2:], y_pred[2:])\n",
    "print('auc_instance_mask', auc.result().numpy())\n",
    "print('mse_instance_mask', mse.result().numpy())\n",
    "print('bce_instance_mask', bce.result().numpy())\n",
    "# 如果利用sample_weight来mask的话，相当于只计算没被mask样本的metric，平均的时候也是对没被mask的样本做平均\n",
    "auc.reset_states()\n",
    "mse.reset_states()\n",
    "bce.reset_states()\n",
    "auc.update_state(y_true, y_pred, sample_weight=np.array([0, 0, 1, 1]))\n",
    "mse.update_state(y_true, y_pred, sample_weight=np.array([0, 0, 1, 1]))\n",
    "bce.update_state(y_true, y_pred, sample_weight=np.array([0, 0, 1, 1]))\n",
    "print('auc_instance_weight', auc.result().numpy())\n",
    "print('auc_instance_weight', mse.result().numpy())\n",
    "print('auc_instance_weight', bce.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc_instance 0.5\n",
      "mse_instance 0.29\n",
      "bce_instance 0.7803236\n",
      "auc_instance 0.6666666\n",
      "mse_instance 0.17\n",
      "bce_instance 0.5056706\n",
      "auc_instance 0.6666666\n",
      "mse_instance 0.17\n",
      "bce_instance 0.5056706\n"
     ]
    }
   ],
   "source": [
    "# 综合来看metric:\n",
    "# 如果每次update_state都是一个batch, 里面含有mask(使用weight实现)\n",
    "# 不断update_state直到一个epoch结束以后的结果，就相当于把这个epoch里面不被mask的样本单独全拎出来做metric计算\n",
    "y_true = [[0], [0], [1], [0]]\n",
    "y_pred = [[0.2], [0.5], [0.3], [0.3]]\n",
    "auc = keras.metrics.AUC()\n",
    "mse = keras.metrics.MeanSquaredError()\n",
    "bce = keras.metrics.BinaryCrossentropy()\n",
    "auc.update_state(y_true, y_pred, sample_weight=np.array([0, 0, 1, 1]))\n",
    "mse.update_state(y_true, y_pred, sample_weight=np.array([0, 0, 1, 1]))\n",
    "bce.update_state(y_true, y_pred, sample_weight=np.array([0, 0, 1, 1]))\n",
    "print('auc_instance', auc.result().numpy())\n",
    "print('mse_instance', mse.result().numpy())\n",
    "print('bce_instance', bce.result().numpy())\n",
    "y_true2 = [[1], [1], [0], [0]]\n",
    "y_pred2 = [[0.8], [0.5], [0.3], [0.1]]\n",
    "auc.update_state(y_true2, y_pred2, sample_weight=np.array([0, 0, 1, 1]))\n",
    "mse.update_state(y_true2, y_pred2, sample_weight=np.array([0, 0, 1, 1]))\n",
    "bce.update_state(y_true2, y_pred2, sample_weight=np.array([0, 0, 1, 1]))\n",
    "print('auc_instance', auc.result().numpy())\n",
    "print('mse_instance', mse.result().numpy())\n",
    "print('bce_instance', bce.result().numpy())\n",
    "auc.reset_states()\n",
    "mse.reset_states()\n",
    "bce.reset_states()\n",
    "auc.update_state(y_true+y_true2, y_pred+y_pred2, sample_weight=np.array([0, 0, 1, 1, 0, 0, 1, 1]))\n",
    "mse.update_state(y_true+y_true2, y_pred+y_pred2, sample_weight=np.array([0, 0, 1, 1, 0, 0, 1, 1]))\n",
    "bce.update_state(y_true+y_true2, y_pred+y_pred2, sample_weight=np.array([0, 0, 1, 1, 0, 0, 1, 1]))\n",
    "print('auc_instance', auc.result().numpy())\n",
    "print('mse_instance', mse.result().numpy())\n",
    "print('bce_instance', bce.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn_auc 0.78125\n",
      "sklearn_ks 0.5\n",
      "sklearn_mse 0.19\n",
      "sklearn_auc_mask 0.888888888888889\n",
      "sklearn_ks_mask 0.6666666666666667\n",
      "sklearn_mse_mask 0.14\n",
      "sklearn_auc_weight 0.888888888888889\n",
      "sklearn_ks_weight 0.6666666666666667\n",
      "sklearn_mse_weight 0.14\n"
     ]
    }
   ],
   "source": [
    "# sklearn和keras里面的metric是一样的逻辑，mask的样本不参与分母部分\n",
    "y_true = [[1], [1], [0], [0], [0], [1], [1], [0]]\n",
    "y_pred = [[0.8], [0.5], [0.3], [0.1], [0.6], [0.7], [0.2], [0.2]]\n",
    "auc_score = roc_auc_score(y_true, y_pred)\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "ks = np.max(np.abs(tpr - fpr))\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print('sklearn_auc', auc_score)\n",
    "print('sklearn_ks', ks)\n",
    "print('sklearn_mse', mse)\n",
    "\n",
    "auc_score = roc_auc_score(y_true[:-2], y_pred[:-2])\n",
    "fpr, tpr, _ = roc_curve(y_true[:-2], y_pred[:-2])\n",
    "ks = np.max(np.abs(tpr - fpr))\n",
    "mse = mean_squared_error(y_true[:-2], y_pred[:-2])\n",
    "print('sklearn_auc_mask', auc_score)\n",
    "print('sklearn_ks_mask', ks)\n",
    "print('sklearn_mse_mask', mse)\n",
    "\n",
    "auc_score = roc_auc_score(y_true, y_pred, sample_weight=np.array([1, 1, 1, 1, 1, 1, 0, 0]))\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred, sample_weight=np.array([1, 1, 1, 1, 1, 1, 0, 0]))\n",
    "ks = np.max(np.abs(tpr - fpr))\n",
    "mse = mean_squared_error(y_true, y_pred, sample_weight=np.array([1, 1, 1, 1, 1, 1, 0, 0]))\n",
    "print('sklearn_auc_weight', auc_score)\n",
    "print('sklearn_ks_weight', ks)\n",
    "print('sklearn_mse_weight', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, None]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=56.0>] [<tf.Tensor: shape=(), dtype=float32, numpy=3.0>]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "例2：求导\n",
    "   1. 常数需要手动watch才能求导，否则为None\n",
    "   2. 如果y不依赖于z,则导数为None\n",
    "   3. 如果需要反复求导，需要在tape里面加persist\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.0)\n",
    "z = tf.Variable(4.0)\n",
    "y = tf.multiply(x, 2)\n",
    "# x.assign(z)\n",
    "with tf.GradientTape() as g:\n",
    "  y = tf.identity(y)\n",
    "dy_dx = g.gradient(y, [x, z])\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.constant(3.0)\n",
    "z = tf.constant(56.0)\n",
    "with tf.GradientTape() as g:\n",
    "  y = x * x\n",
    "dy_dx = g.gradient(y, [x, z])\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  g.watch(z)\n",
    "  y = x * x\n",
    "dy_dx = g.gradient(y, [x, z])\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as g:\n",
    "  g.watch(x)\n",
    "  g.watch(z)\n",
    "  y = x * z\n",
    "dy_dx = g.gradient(y, [x])\n",
    "dy_dz = g.gradient(y, [z])\n",
    "print(dy_dx, dy_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, None]\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "z = x * 3\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  y = x * z\n",
    "# 求导的分母，必须是能通过依赖关系推导得到底层的variable\n",
    "# dy/dx: x是底层variable，所以可以进行求导。但是z的依赖关系在tape里面没有出现，因此dy/dx只能给到z, z=3*x=6\n",
    "# dy/dz：z不是底层底层variable，而且它依赖关系在tape外，无法得到，因此求导为None\n",
    "dy_dx = g.gradient(y, [x, z])\n",
    "print(dy_dx)\n",
    "# 求导的分子，必须是在tape里面定义的变量\n",
    "# dz/dx: x是底层variale，但是z不是在taple定义的（虽然在tape里面的表达式里出现过），求导为None\n",
    "dz_dx = g.gradient(z, [x])\n",
    "print(dz_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=12.0>, <tf.Tensor: shape=(), dtype=float32, numpy=2.0>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=3.0>]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  z = x * 3\n",
    "  y = x * z\n",
    "# 求导的分母，必须是能通过依赖关系推导得到底层的variable\n",
    "# dy/dx: x是底层variable，所以可以进行求导。并且z的依赖关系在tape里面是有的，因此dy/dx=d(x*x*3)/dx=6x=12\n",
    "# dy/dz：z不是底层variable，但依赖关系在tape里面可以得到底层variable x，因此求导为x = 2\n",
    "dy_dx = g.gradient(y, [x, z])\n",
    "print(dy_dx)\n",
    "# 求导的分子，必须是在tape里面定义的变量\n",
    "# dz/dx: x是底层variale，z是在taple定义的，求导为3\n",
    "dz_dx = g.gradient(z, [x])\n",
    "print(dz_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, <tf.Tensor: shape=(), dtype=float32, numpy=2.0>]\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  z = x * 3\n",
    "  # 注意这里必须返回一个同名变量，否则相当于不起效果\n",
    "  z = tf.stop_gradient(z)\n",
    "  y = x * z\n",
    "\n",
    "# 求导的分母，必须是能通过依赖关系推导得到底层的variable\n",
    "# dy/dx: x是底层variable，所以可以进行求导。z的依赖关系在tape里面, 但是z不能对任何变量求导，因此dy/dx=z=3*x=6\n",
    "# dy/dz：z不是底层variable，但依赖关系在tape里面可以得到底层variable x，因此求导为x = 2\n",
    "dy_dx = g.gradient(y, [x, z])\n",
    "print(dy_dx)\n",
    "# 求导的分子，必须是在tape里面定义的变量\n",
    "# dz/dx: x是底层variale，z是在taple定义的，但是z不能对任何变量求导，因此为None\n",
    "dz_dx = g.gradient(z, [x])\n",
    "print(dz_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, <tf.Tensor: shape=(), dtype=float32, numpy=2.0>]\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "f = []\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  z = x * 3\n",
    "  # 注意这里必须返回一个同名变量，否则相当于不起效果\n",
    "  z = tf.stop_gradient(z)\n",
    "  f.append(x * z)\n",
    "\n",
    "# 求导的分子，必须是在tape里面定义过\n",
    "# f[0]元素也是在tape里面定义的，因此也可以求导，效果同上\n",
    "dy_dx = g.gradient(f[0], [x, z])\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=3.0>, <tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, None]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: shape=(), dtype=float32, numpy=1.0>, None]\n",
      "[None, None]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=27.0>, <tf.Tensor: shape=(), dtype=float32, numpy=3.0>]\n",
      "[None, None]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0>, <tf.Tensor: shape=(), dtype=float32, numpy=-0.33333334>]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "j = tf.Variable(2.0)\n",
    "f = []\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  z = x * 3\n",
    "  f.append(z)\n",
    "  k = x * 6\n",
    "  f.append(k)\n",
    "  # 得到shape = (2,)的tensor\n",
    "  ff = tf.stack(f)\n",
    "  # 得到shape = (3,2)的tensor\n",
    "  ll = tf.stack([ff, ff, ff])\n",
    "  kz = k / z\n",
    "\n",
    "# 求导的分子，必须是在tape里面定义过\n",
    "# f[0]元素也是在tape里面定义的，因此也可以求导\n",
    "dy_dx = g.gradient(f[0], [x, z])\n",
    "print(dy_dx)\n",
    "# f[1]元素也是在tape里面定义的，因此也可以求导\n",
    "dy_dx = g.gradient(f[1], [x, z])\n",
    "print(dy_dx)\n",
    "# 如果分子为列表，则对x求导的结果为列表里每个元素求导结果之和，如果某个元素不可导相当于为0, 除非所有元素都不可导则为None\n",
    "dy_dx = g.gradient(f, [x, z, j])\n",
    "print(dy_dx)\n",
    "# 在tape外面定义的op是无法求导的，即便用到的东西全是在tape里面定义的\n",
    "dd = tf.stack(f)\n",
    "dy_dx = g.gradient(dd, [x, z])\n",
    "print(dy_dx)\n",
    "# 与list求导类似，对x求导结果是shape为(2,)的tensor的所有元素求导的结果之和\n",
    "dy_dx = g.gradient(ff, [x, z])\n",
    "print(dy_dx)\n",
    "# 与list求导类似，对x求导结果是shape为(3,2)的tensor的所有元素求导的结果之和\n",
    "dy_dx = g.gradient(ll, [x, z])\n",
    "print(dy_dx)\n",
    "# 在tape外面定义的任何操作是无法求导的，即便用到的东西全是在tape里面定义的\n",
    "dy_dx = g.gradient(k / z, [x, z])\n",
    "print(dy_dx)\n",
    "# 自己推导一下\n",
    "dy_dx = g.gradient(kz, [x, z])\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 二阶导数的例子\n",
    "x = tf.constant(5.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  g.watch(x)\n",
    "  with tf.GradientTape() as gg:\n",
    "    gg.watch(x)\n",
    "    y = x * x\n",
    "  dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\n",
    "d2y_dx2 = g.gradient(dy_dx, x) # d2y_dx2 = 2\n",
    "dy_dx2 = g.gradient(y, x)  # dy_dx = 2 * x\n",
    "print(dy_dx)\n",
    "print(dy_dx2)\n",
    "print(d2y_dx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(10.0, shape=(), dtype=float32)\n",
      "[None, None]\n"
     ]
    }
   ],
   "source": [
    "# 如果不用两个tape的话\n",
    "x = tf.Variable(5.0)\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  y = x * x\n",
    "dy_dx = g.gradient(y, x)\n",
    "print(dy_dx)\n",
    "\n",
    "# 第一个grad dy_dx现在对于第二个tape来说，就是一个constant, 无法求导\n",
    "with tf.GradientTape(persistent=True) as gg:\n",
    "  dd = dy_dx * 10\n",
    "d2y_dx2 = gg.gradient(dd, [x, dy_dx])\n",
    "print(d2y_dx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=6.0>, <tf.Tensor: shape=(), dtype=float32, numpy=2.0>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>, None]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "f = []\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  z = x * 3\n",
    "  # 注意这里必须返回一个同名变量，否则相当于不起效果\n",
    "  z = tf.stop_gradient(z)\n",
    "  f.append(x * z)\n",
    "\n",
    "# 求导的分子，必须是在tape里面定义过\n",
    "# f[0]元素也是在tape里面定义的，因此也可以求导，效果同上\n",
    "dy_dx = g.gradient(f[0], [x, z])\n",
    "print(dy_dx)\n",
    "\n",
    "# 对于第二个tape来说，dy_dx是一个shape为(2,)的constant, 它乘以变量x可以求导了\n",
    "with tf.GradientTape(persistent=True) as g:\n",
    "  ff = dy_dx * x\n",
    "dy_dx = g.gradient(ff, [x, z])\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(40.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 求导是计算的当前值上的导数，可以通过assign改变当前值\n",
    "x = tf.Variable(5.0)\n",
    "with tf.GradientTape() as g:\n",
    "  x.assign(10.0)\n",
    "  x.assign(20.0)\n",
    "  y = x * x\n",
    "dy_dx = g.gradient(y, x)\n",
    "print(dy_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "lr 0.001\n",
      "self.losses [0.0429790951] loss0 0.310979337 loss1 0.89723891 my_total 7.60930872\n",
      "total 7.65228844\n",
      "total——after [0]\n",
      "mse0_metric 0.310979337 mse1_metric 0.89723891\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.6523 - dense_2_loss: 0.0986 - dense_3_loss: 0.7511 - dense_2_mse: 0.3110 - dense_3_mse: 0.8972 - my_mse0: 0.3110 - my_mse1: 0.8972\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/5\n",
      "lr 0.001\n",
      "self.losses [0.0429791026] loss0 0.310979366 loss1 0.89723891 my_total 7.60930872\n",
      "total 7.65228796\n",
      "total——after [-0.00765228458]\n",
      "mse0_metric 0.310979366 mse1_metric 0.89723891\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.6523 - dense_2_loss: 0.0986 - dense_3_loss: 0.7511 - dense_2_mse: 0.3110 - dense_3_mse: 0.8972 - my_mse0: 0.3110 - my_mse1: 0.8972\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/5\n",
      "lr 0.001\n",
      "self.losses [0.0429791026] loss0 0.312579721 loss1 0.900640666 my_total 7.63612366\n",
      "total 7.67910242\n",
      "total——after [-0.0153581752]\n",
      "mse0_metric 0.312579721 mse1_metric 0.900640666\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.6791 - dense_2_loss: 0.0992 - dense_3_loss: 0.7537 - dense_2_mse: 0.3126 - dense_3_mse: 0.9006 - my_mse0: 0.3126 - my_mse1: 0.9006\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/5\n",
      "lr 0.001\n",
      "self.losses [0.0429790951] loss0 0.314385 loss1 0.904436409 my_total 7.66604424\n",
      "total 7.709023\n",
      "total——after [-0.0231280047]\n",
      "mse0_metric 0.314385 mse1_metric 0.904436409\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.7090 - dense_2_loss: 0.0999 - dense_3_loss: 0.7566 - dense_2_mse: 0.3144 - dense_3_mse: 0.9044 - my_mse0: 0.3144 - my_mse1: 0.9044\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/5\n",
      "lr 0.001\n",
      "self.losses [0.0429790951] loss0 0.31629777 loss1 0.908437252 my_total 7.69757509\n",
      "total 7.74055433\n",
      "total——after [-0.0309657231]\n",
      "mse0_metric 0.31629777 mse1_metric 0.908437252\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 7.7406 - dense_2_loss: 0.1007 - dense_3_loss: 0.7597 - dense_2_mse: 0.3163 - dense_3_mse: 0.9084 - my_mse0: 0.3163 - my_mse1: 0.9084\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17e28a37ac8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "例3： loss和metrics在底层是如何计算的\n",
    "'''\n",
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'rate': self.rate}\n",
    "        base_config = super(ActivityRegularizationLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "  def __init__(self, *args, **kargs):\n",
    "      super(CustomModel, self).__init__(*args, **kargs)\n",
    "      self.log_vars= self.add_weight(name='log_var',\n",
    "                                     shape=(1,),\n",
    "                                     initializer=Constant(0.0),\n",
    "                                     trainable=True)\n",
    "\n",
    "  def compile(self, optimizer, loss, metrics, loss_weights):\n",
    "    # 如果super.compile的时候指定了loss，\n",
    "    #    self.compiled_loss执行后self.compiled_loss就变成了对应的metric的list\n",
    "    #    同时self.metrics也会添加对应的metric\n",
    "    # 如果super.compile的时候指定了metric,\n",
    "    #    self.compiled_metric.update_state的时候self.compiled_metric就变成了对应的metric的list\n",
    "    #    同时self.metrics也会添加对应的metric\n",
    "    super(CustomModel, self).compile(optimizer=optimizer, loss=loss, loss_weights=loss_weights, metrics=metrics)\n",
    "    # 下面两句执行后，self.metrics就会添加对应的metric, 但是他们不属于self.compiled_metric\n",
    "    self.mae_metric0 = keras.metrics.MeanSquaredError(name=\"my_mse0\")\n",
    "    self.mae_metric1 = keras.metrics.MeanSquaredError(name=\"my_mse1\")\n",
    "\n",
    "\n",
    "  def train_step(self, data):\n",
    "    x, y, weight = data\n",
    "    with tf.GradientTape() as tape:\n",
    "      tf.print('lr', self.optimizer.lr)\n",
    "      # 前向传播后产生模型中所有regularization、add_loss方法产生的loss，结果作为list保存在self.losses里面\n",
    "      # 每个元素都是一个scalar\n",
    "      #\n",
    "      # 模型实例compile的时候指定的loss，是用于output产生的y_pred与y进行损失计算的，但是这些结果都不含在self.losses里面。\n",
    "      # 我们在compile的时候传入了loss={'a': \"mse\", 'b': \"mse\"}，\n",
    "      # self.compiled_loss调用结束后，self.compiled_loss变成含3个metic的list，分别对应了a的loss， b的loss, 和总体的loss\n",
    "      # compiled_loss调用的时候会对数据结构做归一化，比如y_pred是含两个output的list， sample_weight是一个\n",
    "      # 含有两个vector的字典或者sample_weight就是一个vector，最终都会被归一化成含两个weight vector的list。\n",
    "      # 但是如果sample_weight是一个含有三个vector的字典或者list，只有前两个会被使用。\n",
    "      # compiled_loss调用结束返回的是一个scalar：上述所有loss的和（regularization_losses=self.losses的时候）\n",
    "      # 下面我们手动验证一下\n",
    "      y_pred = self(x, training=True)\n",
    "      mse_loss0 = keras.losses.mean_squared_error(y['a'], y_pred['a']) # (None,)\n",
    "      mse_loss1 = keras.losses.mean_squared_error(y['b'], y_pred['b']) # (None,)\n",
    "      tf.print('self.losses', self.losses,\n",
    "               # 默认batch_size是32, 而我们样本只有3个，所以一个batch就是一个epoch，\n",
    "               # 而每个epoch的metric都会被自动重置，所以\n",
    "               # 一个batch的loss0应该和mse0_metric结果一样， loss1应该和mse1_metric结果一样\n",
    "               'loss0', tf.reduce_mean(mse_loss0), 'loss1', tf.reduce_mean(mse_loss1),\n",
    "               'my_total', tf.reduce_mean(1 * mse_loss0 * weight['a'] + 10 * mse_loss1 * weight['b']))\n",
    "      loss = self.compiled_loss(y, y_pred, sample_weight=weight, regularization_losses=self.losses)\n",
    "      # total = my_total + self.losses(也就是compiles_loss里面通过regularization_losses来进行操作)\n",
    "      tf.print('total', loss)\n",
    "      loss *= self.log_vars\n",
    "      # 最终train_loop自动print出来的的loss不是这个，\n",
    "      # return {m.name: m.result() for m in self.metrics} 才是决定train_loop print什么内容的\n",
    "      # 注意train_loop 自动print的每个输出的loss是加权的，比如keras.losses.mean_squared_error(y['a'], y_pred['a'], weight['a'])\n",
    "      tf.print('total——after', loss)\n",
    "\n",
    "    trainable_vars = self.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    # 模型实例compile的时候如果指定了metric，则这个时候self.compiled_metrics会变成对应的metrics list，\n",
    "    # 但是def compile里面自己定义的self.mae_metric0和self.mae_metric1不包含在self.compiled_metrics里面\n",
    "    self.compiled_metrics.update_state(y, y_pred, sample_weight=weight)\n",
    "    # 因此这里需要将自己定义的self.mae_metric0和self.mae_metric1 update\n",
    "    self.mae_metric0.update_state(y['a'], y_pred['a'])\n",
    "    self.mae_metric1.update_state(y['b'], y_pred['b'])\n",
    "    tf.print('mse0_metric', self.mae_metric0.result(), 'mse1_metric', self.mae_metric1.result())\n",
    "    # 上面已经讲过，self.compiled_loss会变成metric list去跟踪loss, 但是我们不需要去管他们，他们是自动的\n",
    "    # self.metrics包含三部分内容，\n",
    "    #    一部分是self.compiled_loss自己生成metrics,\n",
    "    #    一部分是self.compiled_metrics生成的metrics\n",
    "    #    一部分是def compile里面自己添加的metrics(他们虽然在def compile里面定义，但是不输入self.compiled_metrics)\n",
    "    return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "outputs = ActivityRegularizationLayer()(inputs)\n",
    "outputs1 = keras.layers.Dense(1)(outputs)\n",
    "outputs2 = keras.layers.Dense(1)(outputs)\n",
    "model = CustomModel(inputs, {'a': outputs1, 'b': outputs2})\n",
    "model.compile(optimizer=\"adam\",\n",
    "              metrics='mse',\n",
    "              loss={'a': \"mse\", 'b': \"mse\"},\n",
    "              loss_weights={'a': 1.0, 'b': 10.0})\n",
    "\n",
    "x = np.random.random((3, 3))\n",
    "y = {'a': np.random.random((3, 1)), 'b': np.random.random((3, 1))}\n",
    "weight = {'a': np.array([1.0,0.0,0.0]),\n",
    "          'b': np.array([0.0,1.0,1.0])}\n",
    "model.fit(x,\n",
    "          y,\n",
    "          epochs=5,\n",
    "          sample_weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "lr 0.001\n",
      "self.losses [0.0497862957] loss0 0.0588801168 loss1 0.001637607 my_total 0.0752561837\n",
      "total 0.125042483\n",
      "total——after [0]\n",
      "mse0_metric 0.111876391 mse1_metric 0.0717143491\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1250 - dense_18_loss: 0.0589 - dense_19_loss: 0.0016 - dense_18_mse: 0.1119 - dense_19_mse: 0.0717 - my_mse0: 0.1119 - my_mse1: 0.0717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/5\n",
      "lr 0.001\n",
      "self.losses [0.0497862957] loss0 0.0588801168 loss1 0.001637607 my_total 0.0752561837\n",
      "total 0.125042483\n",
      "total——after [-0.000125039311]\n",
      "mse0_metric 0.111876391 mse1_metric 0.0717143491\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1250 - dense_18_loss: 0.0589 - dense_19_loss: 0.0016 - dense_18_mse: 0.1119 - dense_19_mse: 0.0717 - my_mse0: 0.1119 - my_mse1: 0.0717\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/5lr 0.001\n",
      "\n",
      "self.losses [0.0497862957] loss0 0.0593088232 loss1 0.00173807808 my_total 0.0766896\n",
      "total 0.1264759\n",
      "total——after [-0.000252945931]\n",
      "mse0_metric 0.111530922 mse1_metric 0.0722881928\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1265 - dense_18_loss: 0.0593 - dense_19_loss: 0.0017 - dense_18_mse: 0.1115 - dense_19_mse: 0.0723 - my_mse0: 0.1115 - my_mse1: 0.0723\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/5\n",
      "lr 0.001\n",
      "self.losses [0.0497862957] loss0 0.0597933643 loss1 0.00185716071 my_total 0.0783649683\n",
      "total 0.128151268\n",
      "total——after [-0.000384496234]\n",
      "mse0_metric 0.111145295 mse1_metric 0.0729351416\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1282 - dense_18_loss: 0.0598 - dense_19_loss: 0.0019 - dense_18_mse: 0.1111 - dense_19_mse: 0.0729 - my_mse0: 0.1111 - my_mse1: 0.0729\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/5\n",
      "lr 0.001\n",
      "self.losses [0.0497862957] loss0 0.0603069179 loss1 0.00199037814 my_total 0.0802107\n",
      "total 0.129997\n",
      "total——after [-0.000520168862]\n",
      "mse0_metric 0.110746801 mse1_metric 0.0736237466\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1300 - dense_18_loss: 0.0603 - dense_19_loss: 0.0020 - dense_18_mse: 0.1107 - dense_19_mse: 0.0736 - my_mse0: 0.1107 - my_mse1: 0.0736\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17e2f8c6400>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'rate': self.rate}\n",
    "        base_config = super(ActivityRegularizationLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "  def __init__(self, *args, **kargs):\n",
    "      super(CustomModel, self).__init__(*args, **kargs)\n",
    "      self.log_vars= self.add_weight(name='log_var',\n",
    "                                     shape=(1,),\n",
    "                                     initializer=Constant(0.0),\n",
    "                                     trainable=True)\n",
    "\n",
    "  def compile(self, optimizer, loss, metrics, loss_weights):\n",
    "    # 如果super.compile的时候指定了loss，\n",
    "    #    self.compiled_loss执行后self.compiled_loss就变成了对应的metric的list\n",
    "    #    同时self.metrics也会添加对应的metric\n",
    "    # 如果super.compile的时候指定了metric,\n",
    "    #    self.compiled_metric.update_state的时候self.compiled_metric就变成了对应的metric的list\n",
    "    #    同时self.metrics也会添加对应的metric\n",
    "    super(CustomModel, self).compile(optimizer=optimizer, loss=loss, loss_weights=loss_weights, metrics=metrics)\n",
    "    # 下面两句执行后，self.metrics就会添加对应的metric, 但是他们不属于self.compiled_metric\n",
    "    self.mae_metric0 = keras.metrics.MeanSquaredError(name=\"my_mse0\")\n",
    "    self.mae_metric1 = keras.metrics.MeanSquaredError(name=\"my_mse1\")\n",
    "\n",
    "\n",
    "  def train_step(self, data):\n",
    "    x, y, weight = data\n",
    "    with tf.GradientTape() as tape:\n",
    "      tf.print('lr', self.optimizer.lr)\n",
    "      # 前向传播后产生模型中所有regularization、add_loss方法产生的loss，结果作为list保存在self.losses里面\n",
    "      # 每个元素都是一个scalar\n",
    "      #\n",
    "      # 模型实例compile的时候指定的loss，是用于output产生的y_pred与y进行损失计算的，但是这些结果都不含在self.losses里面。\n",
    "      # 我们在compile的时候传入了loss={'a': \"mse\", 'b': \"mse\"}，\n",
    "      # self.compiled_loss调用结束后，self.compiled_loss变成含3个metic的list，分别对应了a的loss， b的loss, 和总体的loss\n",
    "      # compiled_loss调用的时候会对数据结构做归一化，比如y_pred是含两个output的list， sample_weight是一个\n",
    "      # 含有两个vector的字典或者sample_weight就是一个vector，最终都会被归一化成含两个weight vector的list。\n",
    "      # 但是如果sample_weight是一个含有三个vector的字典或者list，只有前两个会被使用。\n",
    "      # compiled_loss调用结束返回的是一个scalar：上述所有loss的和（regularization_losses=self.losses的时候）\n",
    "      # 下面我们手动验证一下\n",
    "      y_pred = self(x, training=True)\n",
    "      mse_loss0 = keras.losses.MeanSquaredError()(y['a'], y_pred['a'], weight['a']) # scalar\n",
    "      mse_loss1 = keras.losses.MeanSquaredError()(y['b'], y_pred['b'], weight['b']) # scalar\n",
    "      tf.print('self.losses', self.losses,\n",
    "               # 默认batch_size是32, 而我们样本只有3个，所以一个batch就是一个epoch，\n",
    "               # 而每个epoch的metric都会被自动重置，所以\n",
    "               # 一个batch的loss0应该和mse0_metric结果一样， loss1应该和mse1_metric结果一样\n",
    "               'loss0', mse_loss0, 'loss1', mse_loss1,\n",
    "               'my_total', 1 * mse_loss0 + 10 * mse_loss1)\n",
    "      loss = self.compiled_loss(y, y_pred, sample_weight=weight, regularization_losses=self.losses)\n",
    "      # total = my_total + self.losses(也就是compiles_loss里面通过regularization_losses来进行操作)\n",
    "      tf.print('total', loss)\n",
    "      loss *= self.log_vars\n",
    "      # 最终train_loop自动print出来的的loss不是这个，\n",
    "      # return {m.name: m.result() for m in self.metrics} 才是决定train_loop print什么内容的\n",
    "      # 注意train_loop 自动print的每个输出的loss是加权的，比如keras.losses.mean_squared_error(y['a'], y_pred['a'], weight['a'])\n",
    "      tf.print('total——after', loss)\n",
    "\n",
    "    trainable_vars = self.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    # 模型实例compile的时候如果指定了metric，则这个时候self.compiled_metrics会变成对应的metrics list，\n",
    "    # 但是def compile里面自己定义的self.mae_metric0和self.mae_metric1不包含在self.compiled_metrics里面\n",
    "    self.compiled_metrics.update_state(y, y_pred, sample_weight=weight)\n",
    "    # 因此这里需要将自己定义的self.mae_metric0和self.mae_metric1 update\n",
    "    self.mae_metric0.update_state(y['a'], y_pred['a'])\n",
    "    self.mae_metric1.update_state(y['b'], y_pred['b'])\n",
    "    tf.print('mse0_metric', self.mae_metric0.result(), 'mse1_metric', self.mae_metric1.result())\n",
    "    # 上面已经讲过，self.compiled_loss会变成metric list去跟踪loss, 但是我们不需要去管他们，他们是自动的\n",
    "    # self.metrics包含三部分内容，\n",
    "    #    一部分是self.compiled_loss自己生成metrics,\n",
    "    #    一部分是self.compiled_metrics生成的metrics\n",
    "    #    一部分是def compile里面自己添加的metrics(他们虽然在def compile里面定义，但是不输入self.compiled_metrics)\n",
    "    return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "outputs = ActivityRegularizationLayer()(inputs)\n",
    "outputs1 = keras.layers.Dense(1)(outputs)\n",
    "outputs2 = keras.layers.Dense(1)(outputs)\n",
    "model = CustomModel(inputs, {'a': outputs1, 'b': outputs2})\n",
    "model.compile(optimizer=\"adam\",\n",
    "              metrics='mse',\n",
    "              loss={'a': \"mse\", 'b': \"mse\"},\n",
    "              loss_weights={'a': 1.0, 'b': 10.0})\n",
    "\n",
    "x = np.random.random((3, 3))\n",
    "y = {'a': np.random.random((3, 1)), 'b': np.random.random((3, 1))}\n",
    "weight = {'a': np.array([1.0,0.0,0.0]),\n",
    "          'b': np.array([0.0,1.0,1.0])}\n",
    "model.fit(x,\n",
    "          y,\n",
    "          epochs=5,\n",
    "          sample_weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "lr 0.001\n",
      "self.losses [0.0504987575] loss0 0.310474128 loss1 0.996884346 my_total 3.65924668\n",
      "total 7.32167816\n",
      "total——after [0]\n",
      "mse0_metric 0.310474128 mse1_metric 0.996884346\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3217 - dense_20_loss: 0.0473 - dense_21_loss: 0.7224 - dense_20_mse: 0.3105 - dense_21_mse: 0.9969 - my_mse0: 0.3105 - my_mse1: 0.9969\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/5\n",
      "lr 0.001\n",
      "self.losses [0.0504987612] loss0 0.310474128 loss1 0.996884346 my_total 3.65924668\n",
      "total 7.32167816\n",
      "total——after [-0.00732167531]\n",
      "mse0_metric 0.310474128 mse1_metric 0.996884346\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 7.3217 - dense_20_loss: 0.0473 - dense_21_loss: 0.7224 - dense_20_mse: 0.3105 - dense_21_mse: 0.9969 - my_mse0: 0.3105 - my_mse1: 0.9969\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/5\n",
      "lr 0.001\n",
      "self.losses [0.0504987612] loss0 0.312628776 loss1 1.00087273 my_total 3.67461658\n",
      "total 7.35201406\n",
      "total——after [-0.0147039993]\n",
      "mse0_metric 0.312628776 mse1_metric 1.00087273\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.3520 - dense_20_loss: 0.0477 - dense_21_loss: 0.7254 - dense_20_mse: 0.3126 - dense_21_mse: 1.0009 - my_mse0: 0.3126 - my_mse1: 1.0009\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/5\n",
      "lr 0.001\n",
      "self.losses [0.0504987575] loss0 0.315071195 loss1 1.00532281 my_total 3.69177365\n",
      "total 7.38587046\n",
      "total——after [-0.0221586693]\n",
      "mse0_metric 0.315071195 mse1_metric 1.00532281\n",
      "1/1 [==============================] - 0s 965us/step - loss: 7.3859 - dense_20_loss: 0.0482 - dense_21_loss: 0.7287 - dense_20_mse: 0.3151 - dense_21_mse: 1.0053 - my_mse0: 0.3151 - my_mse1: 1.0053\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/5\n",
      "lr 0.001\n",
      "self.losses [0.0504987612] loss0 0.31766212 loss1 1.01001394 my_total 3.70986485\n",
      "total 7.42156649\n",
      "total——after [-0.0296902321]\n",
      "mse0_metric 0.31766212 mse1_metric 1.01001394\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.4216 - dense_20_loss: 0.0487 - dense_21_loss: 0.7322 - dense_20_mse: 0.3177 - dense_21_mse: 1.0100 - my_mse0: 0.3177 - my_mse1: 1.0100\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17e2ff8e6d8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'rate': self.rate}\n",
    "        base_config = super(ActivityRegularizationLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "  def __init__(self, *args, **kargs):\n",
    "      super(CustomModel, self).__init__(*args, **kargs)\n",
    "      self.log_vars= self.add_weight(name='log_var',\n",
    "                                     shape=(1,),\n",
    "                                     initializer=Constant(0.0),\n",
    "                                     trainable=True)\n",
    "\n",
    "  def compile(self, optimizer, loss, metrics, loss_weights):\n",
    "    # 如果super.compile的时候指定了loss，\n",
    "    #    self.compiled_loss执行后self.compiled_loss就变成了对应的metric的list\n",
    "    #    同时self.metrics也会添加对应的metric\n",
    "    # 如果super.compile的时候指定了metric,\n",
    "    #    self.compiled_metric.update_state的时候self.compiled_metric就变成了对应的metric的list\n",
    "    #    同时self.metrics也会添加对应的metric\n",
    "    super(CustomModel, self).compile(optimizer=optimizer, loss=loss, loss_weights=loss_weights, metrics=metrics)\n",
    "    # 下面两句执行后，self.metrics就会添加对应的metric, 但是他们不属于self.compiled_metric\n",
    "    self.mae_metric0 = keras.metrics.MeanSquaredError(name=\"my_mse0\")\n",
    "    self.mae_metric1 = keras.metrics.MeanSquaredError(name=\"my_mse1\")\n",
    "\n",
    "\n",
    "  def train_step(self, data):\n",
    "    x, y, weight = data\n",
    "    with tf.GradientTape() as tape:\n",
    "      tf.print('lr', self.optimizer.lr)\n",
    "      # 前向传播后产生模型中所有regularization、add_loss方法产生的loss，结果作为list保存在self.losses里面\n",
    "      # 每个元素都是一个scalar\n",
    "      #\n",
    "      # 模型实例compile的时候指定的loss，是用于output产生的y_pred与y进行损失计算的，但是这些结果都不含在self.losses里面。\n",
    "      # 我们在compile的时候传入了loss={'a': \"mse\", 'b': \"mse\"}，\n",
    "      # self.compiled_loss调用结束后，self.compiled_loss变成含3个metic的list，分别对应了a的loss， b的loss, 和总体的loss\n",
    "      # compiled_loss调用的时候会对数据结构做归一化，比如y_pred是含两个output的list， sample_weight是一个\n",
    "      # 含有两个vector的字典或者sample_weight就是一个vector，最终都会被归一化成含两个weight vector的list。\n",
    "      # 但是如果sample_weight是一个含有三个vector的字典或者list，只有前两个会被使用。\n",
    "      # compiled_loss调用结束返回的是一个scalar：上述所有loss的和（regularization_losses=self.losses的时候）\n",
    "      # 下面我们手动验证一下\n",
    "      y_pred = self(x, training=True)\n",
    "      mse_loss0 = keras.losses.mean_squared_error(y['a'], y_pred['a']) # (None,)\n",
    "      mse_loss1 = keras.losses.mean_squared_error(y['b'], y_pred['b']) # (None,)\n",
    "      tf.print('self.losses', self.losses,\n",
    "               # 默认batch_size是32, 而我们样本只有3个，所以一个batch就是一个epoch，\n",
    "               # 而每个epoch的metric都会被自动重置，所以\n",
    "               # 一个batch的loss0应该和mse0_metric结果一样， loss1应该和mse1_metric结果一样\n",
    "               'loss0', tf.reduce_mean(mse_loss0), 'loss1', tf.reduce_mean(mse_loss1),\n",
    "               'my_total', tf.reduce_mean(1 * mse_loss0 * weight['a'] / tf.reduce_sum(weight['a']) +\n",
    "                                          10 * mse_loss1 * weight['b'] / tf.reduce_sum(weight['b'])))\n",
    "      loss = self.compiled_loss(y, y_pred, sample_weight=weight, regularization_losses=self.losses)\n",
    "      # total = my_total + self.losses(也就是compiles_loss里面通过regularization_losses来进行操作)\n",
    "      tf.print('total', loss)\n",
    "      loss *= self.log_vars\n",
    "      # 最终train_loop自动print出来的的loss不是这个，\n",
    "      # return {m.name: m.result() for m in self.metrics} 才是决定train_loop print什么内容的\n",
    "      # 注意train_loop 自动print的每个输出的loss是加权的，比如keras.losses.mean_squared_error(y['a'], y_pred['a'], weight['a'])\n",
    "      tf.print('total——after', loss)\n",
    "\n",
    "    trainable_vars = self.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    # 模型实例compile的时候如果指定了metric，则这个时候self.compiled_metrics会变成对应的metrics list，\n",
    "    # 但是def compile里面自己定义的self.mae_metric0和self.mae_metric1不包含在self.compiled_metrics里面\n",
    "    self.compiled_metrics.update_state(y, y_pred, sample_weight=weight)\n",
    "    # 因此这里需要将自己定义的self.mae_metric0和self.mae_metric1 update\n",
    "    self.mae_metric0.update_state(y['a'], y_pred['a'])\n",
    "    self.mae_metric1.update_state(y['b'], y_pred['b'])\n",
    "    tf.print('mse0_metric', self.mae_metric0.result(), 'mse1_metric', self.mae_metric1.result())\n",
    "    # 上面已经讲过，self.compiled_loss会变成metric list去跟踪loss, 但是我们不需要去管他们，他们是自动的\n",
    "    # self.metrics包含三部分内容，\n",
    "    #    一部分是self.compiled_loss自己生成metrics,\n",
    "    #    一部分是self.compiled_metrics生成的metrics\n",
    "    #    一部分是def compile里面自己添加的metrics(他们虽然在def compile里面定义，但是不输入self.compiled_metrics)\n",
    "    return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "outputs = ActivityRegularizationLayer()(inputs)\n",
    "outputs1 = keras.layers.Dense(1)(outputs)\n",
    "outputs2 = keras.layers.Dense(1)(outputs)\n",
    "model = CustomModel(inputs, {'a': outputs1, 'b': outputs2})\n",
    "model.compile(optimizer=\"adam\",\n",
    "              metrics='mse',\n",
    "              loss={'a': \"mse\", 'b': \"mse\"},\n",
    "              loss_weights={'a': 1.0, 'b': 10.0})\n",
    "\n",
    "x = np.random.random((3, 3))\n",
    "y = {'a': np.random.random((3, 1)), 'b': np.random.random((3, 1))}\n",
    "weight = {'a': np.array([1.0,0.0,0.0]),\n",
    "          'b': np.array([0.0,1.0,1.0])}\n",
    "model.fit(x,\n",
    "          y,\n",
    "          epochs=5,\n",
    "          sample_weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_22/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.71009797],\n",
      "       [0.12378993],\n",
      "       [0.55186844]], dtype=float32)>, <tf.Variable 'dense_22/bias:0' shape=(1,) dtype=float32, numpy=array([0.00328385], dtype=float32)>, <tf.Variable 'dense_23/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.6119768 ],\n",
      "       [0.5554451 ],\n",
      "       [0.01648403]], dtype=float32)>, <tf.Variable 'dense_23/bias:0' shape=(1,) dtype=float32, numpy=array([-0.00334061], dtype=float32)>, <tf.Variable 'total:0' shape=() dtype=float32, numpy=1.0930122>, <tf.Variable 'count:0' shape=() dtype=float32, numpy=3.0>, <tf.Variable 'total:0' shape=() dtype=float32, numpy=0.310604>, <tf.Variable 'count:0' shape=() dtype=float32, numpy=3.0>, <tf.Variable 'log_var:0' shape=(1,) dtype=float32, numpy=array([-0.00500159], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "例4：自定义模型的复原\n",
    "'''\n",
    "model.save_weights('muy')\n",
    "# 模型的权重是包含两个自定义的metric的,不过没关系，重新训练的时候都会被重置的\n",
    "print(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_22/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.71009797],\n",
      "       [0.12378993],\n",
      "       [0.55186844]], dtype=float32)>, <tf.Variable 'dense_22/bias:0' shape=(1,) dtype=float32, numpy=array([0.00328385], dtype=float32)>, <tf.Variable 'dense_23/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.6119768 ],\n",
      "       [0.5554451 ],\n",
      "       [0.01648403]], dtype=float32)>, <tf.Variable 'dense_23/bias:0' shape=(1,) dtype=float32, numpy=array([-0.00334061], dtype=float32)>, <tf.Variable 'log_var:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "model2 = CustomModel(inputs, {'a': outputs1, 'b': outputs2})\n",
    "# 此时权重是不包含metric的（上面已经讲过，自定义的metric在def compile里面才会生成，因此这个时候是没有他们的权重的）\n",
    "# 但是这个时候有log_var的权重，这个在__ini__里面定义的，所以只要实例化就可以有的，不过是初始值0\n",
    "print(model2.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_22/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.71009797],\n",
      "       [0.12378993],\n",
      "       [0.55186844]], dtype=float32)>, <tf.Variable 'dense_22/bias:0' shape=(1,) dtype=float32, numpy=array([0.00328385], dtype=float32)>, <tf.Variable 'dense_23/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.6119768 ],\n",
      "       [0.5554451 ],\n",
      "       [0.01648403]], dtype=float32)>, <tf.Variable 'dense_23/bias:0' shape=(1,) dtype=float32, numpy=array([-0.00334061], dtype=float32)>, <tf.Variable 'total:0' shape=() dtype=float32, numpy=0.0>, <tf.Variable 'count:0' shape=() dtype=float32, numpy=0.0>, <tf.Variable 'total:0' shape=() dtype=float32, numpy=0.0>, <tf.Variable 'count:0' shape=() dtype=float32, numpy=0.0>, <tf.Variable 'log_var:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer=keras.optimizers.Adam(learning_rate=0.6),\n",
    "               metrics='mse',\n",
    "               loss={'a': \"mse\", 'b': \"mse\"},\n",
    "               loss_weights={'a': 1.0, 'b': 10.0})\n",
    "# 这个时候已经compile过了，可以看到模型产生了自定义的metric的权重，不过是初始值0\n",
    "print(model2.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.6\r\n",
      "self.losses [0.00785260927] loss0 0.216828778 loss1 0.25319913 my_total 25.5367413\r\n",
      "total 25.5445938\r\n",
      "total——after [0]\r\n",
      "mse0_metric 0.216828778 mse1_metric 0.25319913\r\n",
      "[<tf.Variable 'dense_22/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.71009797],\n",
      "       [0.12378993],\n",
      "       [0.55186844]], dtype=float32)>, <tf.Variable 'dense_22/bias:0' shape=(1,) dtype=float32, numpy=array([0.00328385], dtype=float32)>, <tf.Variable 'dense_23/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.6119768 ],\n",
      "       [0.5554451 ],\n",
      "       [0.01648403]], dtype=float32)>, <tf.Variable 'dense_23/bias:0' shape=(1,) dtype=float32, numpy=array([-0.00334061], dtype=float32)>, <tf.Variable 'total:0' shape=() dtype=float32, numpy=1.0930122>, <tf.Variable 'count:0' shape=() dtype=float32, numpy=3.0>, <tf.Variable 'total:0' shape=() dtype=float32, numpy=0.310604>, <tf.Variable 'count:0' shape=() dtype=float32, numpy=3.0>, <tf.Variable 'log_var:0' shape=(1,) dtype=float32, numpy=array([-0.00500159], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "model2.train_on_batch(x[:1],\n",
    "                      {name: value[:1] for name, value in y.items()},\n",
    "                      sample_weight={name: value[:1] for name, value in weight.items()})\n",
    "model2.load_weights('muy')\n",
    "# 这个时候，无论是自定义的metric的权重，log_var的权重，optimizers的权重，全部都复原了！\n",
    "print(model2.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "lr 0.001\r\n",
      "self.losses [0.0397862755] loss0 0.365132928 loss1 0.104212619 my_total 10.786396\r\n",
      "total 10.8261824\r\n",
      "total——after [-0.0541481413]\r\n",
      "mse0_metric 0.365132928 mse1_metric 0.104212619\r\n",
      "1/1 [==============================] - 0s 0s/step - loss: 10.8262 - dense_22_loss: 0.3651 - dense_23_loss: 1.0421 - dense_22_mse: 0.3651 - dense_23_mse: 0.1042 - my_mse0: 0.3651 - my_mse1: 0.1042\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/5\n",
      "lr 0.001\r\n",
      "self.losses [0.0397862718] loss0 0.365959436 loss1 0.104917973 my_total 10.8577576\r\n",
      "total 10.897542\r\n",
      "total——after [-0.0654188097]\r\n",
      "mse0_metric 0.365959436 mse1_metric 0.104917973\r\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.8975 - dense_22_loss: 0.3660 - dense_23_loss: 1.0492 - dense_22_mse: 0.3660 - dense_23_mse: 0.1049 - my_mse0: 0.3660 - my_mse1: 0.1049\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/5\n",
      "lr 0.001\r\n",
      "self.losses [0.0397862755] loss0 0.366814941 loss1 0.105649173 my_total 10.9317312\r\n",
      "total 10.9715185\r\n",
      "total——after [-0.0768581927]\r\n",
      "mse0_metric 0.366814941 mse1_metric 0.105649173\r\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 10.9715 - dense_22_loss: 0.3668 - dense_23_loss: 1.0565 - dense_22_mse: 0.3668 - dense_23_mse: 0.1056 - my_mse0: 0.3668 - my_mse1: 0.1056\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/5\n",
      "lr 0.001\r\n",
      "self.losses [0.0397862755] loss0 0.367698282 loss1 0.106405497 my_total 11.0082474\r\n",
      "total 11.0480337\r\n",
      "total——after [-0.0884751]\r\n",
      "mse0_metric 0.367698282 mse1_metric 0.106405497\r\n",
      "1/1 [==============================] - 0s 0s/step - loss: 11.0480 - dense_22_loss: 0.3677 - dense_23_loss: 1.0641 - dense_22_mse: 0.3677 - dense_23_mse: 0.1064 - my_mse0: 0.3677 - my_mse1: 0.1064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/5\n",
      "lr 0.001\r\n",
      "self.losses [0.0397862755] loss0 0.368608952 loss1 0.107186697 my_total 11.0872774\r\n",
      "total 11.1270638\r\n",
      "total——after [-0.100278527]\r\n",
      "mse0_metric 0.368608952 mse1_metric 0.107186697\r\n",
      "1/1 [==============================] - 0s 0s/step - loss: 11.1271 - dense_22_loss: 0.3686 - dense_23_loss: 1.0719 - dense_22_mse: 0.3686 - dense_23_mse: 0.1072 - my_mse0: 0.3686 - my_mse1: 0.1072\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2283940fd30>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个时候就相当于恢复训练了，可以看到连学习率都复原了，而不是本次初始化时的0.6\n",
    "model2.fit(x,\n",
    "           y,\n",
    "           epochs=5,\n",
    "           sample_weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "例5：自定义模型可以用model.save方法保存吗？\n",
    "目前看来，只要是Model(input, ouput)这种形式的都不行\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
